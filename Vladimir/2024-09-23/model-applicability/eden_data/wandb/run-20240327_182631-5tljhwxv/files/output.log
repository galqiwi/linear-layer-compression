

Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.07it/s]
/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Starting ...
Ready.
  0%|                                                                                                                                                                                                                                                                                       | 0/32 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                        | 0/1 [00:00<?, ?it/s]
0 self_attn.q_proj

  assignments = torch.bucketize(weight, boundaries[bits])                                                                                                                                                                                                                                    | 0/1 [00:00<?, ?it/s]
time 1.60
error 78951.34375
  0%|                                                                                                                                                                                                                                                                                       | 0/32 [00:13<?, ?it/s]
Traceback (most recent call last):
  File "/nfs/scistore19/alistgrp/apanfero/CompressionEntropy/gptq/llama2.py", line 340, in <module>
    quantizers = llama_sequential(model, dataloader, DEV)
  File "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/nfs/scistore19/alistgrp/apanfero/CompressionEntropy/gptq/llama2.py", line 141, in llama_sequential
    quantized_linear = HadLinear(gptq[name].layer.weight, args.groupsize, args.do_hadamard, args.actquant)
AttributeError: 'Namespace' object has no attribute 'do_hadamard'