
config_dict={'_name_or_path': 'meta-llama/Llama-2-7b-hf', 'architectures': ['LlamaForCausalLM'], 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 11008, 'max_position_embeddings': 4096, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'num_key_value_heads': 32, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': None, 'tie_word_embeddings': False, 'torch_dtype': 'float16', 'transformers_version': '4.31.0.dev0', 'use_cache': True, 'vocab_size': 32000, '_commit_hash': '8cca527612d856d7d32bd94f8103728d614eb852'}
Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                          | 0/2 [00:19<?, ?it/s]
Traceback (most recent call last):
  File "/nfs/scistore14/alistgrp/apanfero/CompressionEntropy/gptq/llama2.py", line 302, in <module>
    model = get_llama(args.model)
  File "/nfs/scistore14/alistgrp/apanfero/CompressionEntropy/gptq/llama2.py", line 34, in get_llama
    model = LlamaForCausalLM.from_pretrained(name, torch_dtype='auto')
  File "/nfs/scistore14/alistgrp/apanfero/transformers/src/transformers/modeling_utils.py", line 3504, in from_pretrained
    ) = cls._load_pretrained_model(