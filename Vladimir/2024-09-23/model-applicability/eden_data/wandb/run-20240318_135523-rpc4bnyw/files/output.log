
config_dict={'_name_or_path': 'meta-llama/Llama-2-7b-hf', 'architectures': ['LlamaForCausalLM'], 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 11008, 'max_position_embeddings': 4096, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'num_key_value_heads': 32, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': None, 'tie_word_embeddings': False, 'torch_dtype': 'float16', 'transformers_version': '4.31.0.dev0', 'use_cache': True, 'vocab_size': 32000, '_commit_hash': '8cca527612d856d7d32bd94f8103728d614eb852'}


Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:07<00:00, 33.90s/it]
Traceback (most recent call last):
  File "/nfs/scistore14/alistgrp/apanfero/CompressionEntropy/gptq/llama2.py", line 305, in <module>
    dataloader, testloader = get_loaders(
  File "/nfs/scistore14/alistgrp/apanfero/CompressionEntropy/gptq/datautils.py", line 67, in get_loaders
    return get_red(nsamples, seed, seqlen, model)
  File "/nfs/scistore14/alistgrp/apanfero/CompressionEntropy/gptq/datautils.py", line 41, in get_red
    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)
  File "/nfs/scistore14/alistgrp/apanfero/transformers/src/transformers/models/auto/tokenization_auto.py", line 826, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/nfs/scistore14/alistgrp/apanfero/transformers/src/transformers/utils/import_utils.py", line 1323, in __getattribute__
    requires_backends(cls, cls._backends)
  File "/nfs/scistore14/alistgrp/apanfero/transformers/src/transformers/utils/import_utils.py", line 1311, in requires_backends
    raise ImportError("".join(failed))
ImportError:
LlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.