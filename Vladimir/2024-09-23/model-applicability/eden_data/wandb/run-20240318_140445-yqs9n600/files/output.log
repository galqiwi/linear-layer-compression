
config_dict={'_name_or_path': 'meta-llama/Llama-2-7b-hf', 'architectures': ['LlamaForCausalLM'], 'bos_token_id': 1, 'eos_token_id': 2, 'hidden_act': 'silu', 'hidden_size': 4096, 'initializer_range': 0.02, 'intermediate_size': 11008, 'max_position_embeddings': 4096, 'model_type': 'llama', 'num_attention_heads': 32, 'num_hidden_layers': 32, 'num_key_value_heads': 32, 'pretraining_tp': 1, 'rms_norm_eps': 1e-05, 'rope_scaling': None, 'tie_word_embeddings': False, 'torch_dtype': 'float16', 'transformers_version': '4.31.0.dev0', 'use_cache': True, 'vocab_size': 32000, '_commit_hash': '8cca527612d856d7d32bd94f8103728d614eb852'}

Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.12it/s]
/nfs/scistore14/alistgrp/apanfero/AQLM/.conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample
You can avoid this message in future by passing the argument `trust_remote_code=True`.
Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.
  warnings.warn(
Starting ...
Ready.
  0%|                                                                                                                                                                                                                                                                                    | 0/32 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/nfs/scistore14/alistgrp/apanfero/CompressionEntropy/gptq/llama2.py", line 311, in <module>
    quantizers = llama_sequential(model, dataloader, DEV)
  File "/nfs/scistore14/alistgrp/apanfero/AQLM/.conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/nfs/scistore14/alistgrp/apanfero/CompressionEntropy/gptq/llama2.py", line 116, in llama_sequential
    layer(inps[j], attention_mask=attention_masks[j], position_ids=position_ids[j])
  File "/nfs/scistore14/alistgrp/apanfero/AQLM/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nfs/scistore14/alistgrp/apanfero/AQLM/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/nfs/scistore14/alistgrp/apanfero/transformers/src/transformers/models/llama/modeling_llama.py", line 742, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/nfs/scistore14/alistgrp/apanfero/AQLM/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nfs/scistore14/alistgrp/apanfero/AQLM/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/nfs/scistore14/alistgrp/apanfero/transformers/src/transformers/models/llama/modeling_llama.py", line 673, in forward
    attn_output = torch.nn.functional.scaled_dot_product_attention(
RuntimeError: The expanded size of the tensor (1946) must match the existing size (4096) at non-singleton dimension 3.  Target sizes: [1, 32, 1946, 1946].  Tensor sizes: [1, 1, 4096, 4096]