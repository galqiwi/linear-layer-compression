


Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.62s/it]






Quantizing linear layers...: 100%|██████████████████████████████████████████████████████| 225/225 [00:11<00:00, 19.07it/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors
wikitext2
Evaluating ...
Evaluating layer-by-layer...:   0%|                                                                | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be removed and `position_embeddings` will be mandatory.































Evaluating layer-by-layer...: 100%|███████████████████████████████████████████████████████| 32/32 [01:25<00:00,  2.69s/it]
88.87909698486328