

Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.17s/it]

































Quantizing linear layers...: 100%|██████████████████████████████████████████████████████| 225/225 [01:05<00:00,  3.45it/s]
Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors
wikitext2
Evaluating ...
Evaluating layer-by-layer...:   0%|                                                                | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be removed and `position_embeddings` will be mandatory.































Evaluating layer-by-layer...: 100%|███████████████████████████████████████████████████████| 32/32 [01:26<00:00,  2.71s/it]
14.468684196472168