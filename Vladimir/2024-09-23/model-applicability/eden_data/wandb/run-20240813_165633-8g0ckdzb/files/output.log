


Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.07s/it]









































































Quantizing linear layers...: 100%|██████████████████████████████████████████████████████| 225/225 [02:28<00:00,  1.52it/s]
Using the latest cached version of the dataset since wikitext couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'wikitext-2-raw-v1' at /nfs/scistore19/alistgrp/apanfero/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/0.0.0/b08601e04326c79dfdd32d625aee71d232d685c3 (last modified on Wed Feb 21 10:35:29 2024).
Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors
Evaluating layer-by-layer...:   0%|                                                                | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be removed and `position_embeddings` will be mandatory.
wikitext2
Evaluating ...































Evaluating layer-by-layer...: 100%|███████████████████████████████████████████████████████| 32/32 [01:23<00:00,  2.60s/it]
7.270035743713379