{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=7\n",
      "env: TOKENIZERS_PARALLELISM=false\n",
      "env: WANDB_NOTEBOOK_NAME=quant.ipynb\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=7\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "%env WANDB_NOTEBOOK_NAME=quant.ipynb\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produce tensors scale and zero of shape (m, 1)\n",
    "        such that 0 < x / scale + zero < max_abs\"\"\"\n",
    "    # YOUR CODE HERE>>>>>>>>>\n",
    "    xmin = x.min(-1)[0]\n",
    "    xmax = x.max(-1)[0]\n",
    "\n",
    "    scale = (xmax - xmin) / max_abs\n",
    "    scale[scale == 0] = 1\n",
    "    zero = -xmin / scale\n",
    "\n",
    "    scale = scale.unsqueeze(-1)\n",
    "    zero = zero.unsqueeze(-1)\n",
    "    return scale.to(x.dtype), zero.to(x.dtype)\n",
    "\n",
    "def quantize_simple(x, bits):\n",
    "    return torch.clamp(torch.round(x), 0, 2 ** bits - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadamard quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cache, lru_cache\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "\n",
    "# half-normal centroids\n",
    "opt_hn_centroids = {\n",
    "    1: [0.7978845608028654],\n",
    "    2: [0.4527800398860679, 1.5104176087114887],\n",
    "    3: [0.24509416307340598, 0.7560052489539643, 1.3439092613750225, 2.151945669890335],\n",
    "    4: [\n",
    "        0.12839501671105813,\n",
    "        0.38804823445328507,\n",
    "        0.6567589957631145,\n",
    "        0.9423402689122875,\n",
    "        1.2562309480263467,\n",
    "        1.6180460517130526,\n",
    "        2.069016730231837,\n",
    "        2.732588804065177,\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def section_variance(a, b, c) -> float:\n",
    "    if math.isinf(c):\n",
    "        return (\n",
    "            np.sqrt(2 / np.pi) * np.exp(-(a**2) / 2) * (a - 2 * b)\n",
    "            - (b**2 + 1) * erf(a / np.sqrt(2))\n",
    "            + (b**2 + 1) * erf(c / np.sqrt(2))\n",
    "        )\n",
    "    else:\n",
    "        return (\n",
    "            np.sqrt(2 / np.pi) * np.exp(-(a**2) / 2) * (a - 2 * b)\n",
    "            - (b**2 + 1) * erf(a / np.sqrt(2))\n",
    "            + (b**2 + 1) * erf(c / np.sqrt(2))\n",
    "            + np.sqrt(2 / np.pi) * np.exp(-(c**2) / 2) * (2 * b - c)\n",
    "        )\n",
    "\n",
    "\n",
    "def gen_boundaries(centroids):\n",
    "    return [(a + b) / 2 for a, b in zip(centroids[:-1], centroids[1:])]\n",
    "\n",
    "\n",
    "def gen_all_normal_quantization_constants():\n",
    "    # add symmetric negative normal centroids\n",
    "    centroids = {i: [-j for j in reversed(c)] + c for i, c in opt_hn_centroids.items()}\n",
    "\n",
    "    # centroids to bin boundaries\n",
    "    boundaries = {i: gen_boundaries(c) for i, c in centroids.items()}\n",
    "\n",
    "    return centroids, boundaries\n",
    "    \n",
    "\n",
    "@cache\n",
    "def bits_var():\n",
    "    result = {0: 1}\n",
    "    for bits, centers in opt_hn_centroids.items():\n",
    "        borders = [0] + [(a + b) / 2 for a, b in zip(centers[:-1], centers[1:])] + [float(\"inf\")]\n",
    "        variance = sum(section_variance(a, b, c) for a, b, c in zip(borders[:-1], centers, borders[1:]))\n",
    "        result[bits] = variance\n",
    "    return result\n",
    "\n",
    "\n",
    "@cache\n",
    "def get_all_quantization_constants_tensors(device):\n",
    "    centroids, boundaries = gen_all_normal_quantization_constants()\n",
    "\n",
    "    centroids = {i: torch.tensor(c, device=device) for i, c in centroids.items()}\n",
    "    boundaries = {i: torch.tensor(b, device=device) for i, b in boundaries.items()}\n",
    "\n",
    "    return centroids, boundaries\n",
    "\n",
    "\n",
    "\n",
    "centroids, boundaries = get_all_quantization_constants_tensors(DEVICE)\n",
    "\n",
    "def quantize_hadamard(weight, bits):\n",
    "    assignments = torch.bucketize(weight, boundaries[bits])\n",
    "    return torch.take(centroids[bits], assignments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def gptq_block(block_weight: Tensor, block_hessian_inverse: Tensor, bits: int, do_hadamard: bool) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Perform GPTQ within block\n",
    "    Args:\n",
    "        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n",
    "        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n",
    "        bits (int): number of bits to quantize() to\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n",
    "    \"\"\"\n",
    "    quantized_block_weight = torch.zeros_like(block_weight)\n",
    "    scaled_block_error = torch.zeros_like(block_weight)\n",
    "\n",
    "    # Interate over the block's columns\n",
    "    for i in range(block_weight.shape[1]):\n",
    "        # Get the column and the corresponding inverse Hessian\n",
    "        column_weight = block_weight[:, [i]]\n",
    "        # YOUR CODE HERE>>>>>>>>>\n",
    "        column_hessian_inverse = block_hessian_inverse[i, i]\n",
    "\n",
    "        # Quantize the column weight\n",
    "        if do_hadamard:\n",
    "            quantized_column_weight = quantize_hadamard(column_weight.clone(), bits)\n",
    "        else:\n",
    "            quantized_column_weight = quantize_simple(column_weight.clone(), bits)\n",
    "        quantized_block_weight[:, [i]] = quantized_column_weight.clone()\n",
    "        dequantized_column_weight = quantized_column_weight\n",
    "\n",
    "        # Update all the following columns within the block\n",
    "        scaled_column_error = (column_weight - dequantized_column_weight) / column_hessian_inverse\n",
    "        block_weight[:, i+1:] -= scaled_column_error.matmul(block_hessian_inverse[[i], i+1:])\n",
    "        scaled_block_error[:, [i]] = scaled_column_error\n",
    "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    return quantized_block_weight, scaled_block_error, block_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_hadamard_transform import hadamard_transform\n",
    "\n",
    "BLOCK_SIZE = 1024\n",
    "\n",
    "def prepare_inverse_hessian(hessian: Tensor, percdamp: float) -> Tensor:\n",
    "    \"\"\"Precomputes inverse Hessian\n",
    "    Args:\n",
    "        hessian (Tensor): problem hessian\n",
    "        percdamp (float): diagonal damping constant for numerical stability\n",
    "    Returns:\n",
    "        Tensor: precomputed inverse Hessian\n",
    "    \"\"\"\n",
    "    damp = percdamp * torch.mean(torch.diag(hessian))\n",
    "    diag = torch.arange(hessian.shape[0], device=hessian.device)\n",
    "    hessian[diag, diag] += damp\n",
    "    hessian = torch.linalg.cholesky(hessian)\n",
    "    hessian = torch.cholesky_inverse(hessian)\n",
    "    hessian = torch.linalg.cholesky(hessian, upper=True)\n",
    "    return hessian\n",
    "\n",
    "def pad_to_power_of_2(tensor, dims):\n",
    "    pad_dims = [0 for _ in range(2 * len(tensor.shape))]\n",
    "    for dim in dims:\n",
    "        size = tensor.shape[dim]\n",
    "        next_power_of_2 = 2**(size-1).bit_length()\n",
    "        delta = next_power_of_2 - size\n",
    "        pad_dims[-2 * dim - 1] = delta\n",
    "    \n",
    "    return F.pad(tensor, pad_dims, \"constant\", 0)\n",
    "\n",
    "def pad_to_block(tensor, dims):\n",
    "    pad_dims = [0 for _ in range(2 * len(tensor.shape))]\n",
    "    for dim in dims:\n",
    "        size = tensor.shape[dim]\n",
    "        next_multiple_of_1024 = ((size - 1) // BLOCK_SIZE + 1) * BLOCK_SIZE\n",
    "        delta = next_multiple_of_1024 - size\n",
    "        pad_dims[-2 * dim - 1] = delta\n",
    "    \n",
    "    return F.pad(tensor, pad_dims, \"constant\", 0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def gptq(\n",
    "    weight: torch.Tensor, bits: int, hessian: torch.Tensor,\n",
    "    do_hadamard=True,\n",
    "    blocksize:int=128, percdamp:float=.01\n",
    ") -> tuple[Tensor, Tensor, Tensor]:\n",
    "    \"\"\"Quantizes weight with GPTQ\n",
    "    Args:\n",
    "        weight (torch.Tensor): weight to quantize\n",
    "        bits (int): number of bits to quantize to\n",
    "        hessian (torch.Tensor): problem Hessian\n",
    "        blocksize (int, optional): Defaults to 128.\n",
    "        percdamp (float, optional): Hessian damping constant for numerical stability. Defaults to .01.\n",
    "\n",
    "    Returns:\n",
    "        tuple[Tensor, Tensor, Tensor]: quantized_weight, row-wise quantization scales, row-wise quantization zeroes\n",
    "    \"\"\"\n",
    "\n",
    "    dtype = weight.dtype\n",
    "    weight = weight.float()\n",
    "    num_columns = weight.shape[1]\n",
    "    hessian = hessian.float()\n",
    "\n",
    "    # Normalize\n",
    "    if do_hadamard:\n",
    "        # scales = torch.linalg.norm(weight, axis=-1)\n",
    "        weight = pad_to_block(weight, [1])\n",
    "        hessian = pad_to_block(hessian, [0, 1])\n",
    "        \n",
    "        mult = weight.shape[1] // BLOCK_SIZE\n",
    "        weight = weight.reshape(-1, mult, BLOCK_SIZE)\n",
    "        hessian = hessian.reshape(mult, BLOCK_SIZE, mult, BLOCK_SIZE)\n",
    "        \n",
    "        scales = torch.linalg.norm(weight, axis=-1)\n",
    "        \n",
    "        weight = hadamard_transform(weight) / scales[:, :, None]\n",
    "        hessian = hadamard_transform(\n",
    "            hadamard_transform(hessian, scale=1/np.sqrt(BLOCK_SIZE)).permute(2, 3, 0, 1),\n",
    "            scale=1/np.sqrt(BLOCK_SIZE)\n",
    "        ).permute(2, 3, 0, 1)\n",
    "        \n",
    "        weight = weight.reshape(-1, mult * BLOCK_SIZE)\n",
    "        hessian = hessian.reshape(mult * BLOCK_SIZE, mult * BLOCK_SIZE)\n",
    "    else:\n",
    "        scales, zeros = get_scale_and_zero(weight, 2**bits - 1)\n",
    "        weight = weight / scales + zeros\n",
    "\n",
    "    # Process the Hessian to obtain the precomputed inverse Hessian\n",
    "    hessian_inverse = prepare_inverse_hessian(hessian, percdamp)\n",
    "\n",
    "    # Iterate over the columns in blockss\n",
    "    quantized_weight = weight.clone()\n",
    "    for block_start in range(0, num_columns, blocksize):\n",
    "        # YOUR CODE HERE>>>>>>>>>\n",
    "        block_end = min(block_start + blocksize, num_columns)\n",
    "\n",
    "        # Get the next block and quantize it\n",
    "        quantized_block_weight, block_error, weight[:, block_start:block_end] = gptq_block(\n",
    "            weight[:, block_start:block_end],\n",
    "            hessian_inverse[block_start:block_end, block_start:block_end],\n",
    "            bits, do_hadamard,\n",
    "        )\n",
    "\n",
    "        # Tune all the following blocks to mitigate the quantization error\n",
    "        quantized_weight[:, block_start:block_end] = quantized_block_weight.clone()\n",
    "        weight[:, block_end:] -= block_error.matmul(hessian_inverse[block_start:block_end, block_end:])\n",
    "        # <<<<<<<<<<<<<<<<<<<<<<<\n",
    "    if do_hadamard:\n",
    "        quantized_weight = (quantized_weight.reshape(quantized_weight.shape[0], -1, BLOCK_SIZE) * scales[:, :, None]).reshape(quantized_weight.shape[0], -1)\n",
    "    else:\n",
    "        quantized_weight = (quantized_weight - zeros) * scales\n",
    "    return quantized_weight.to(dtype)\n",
    "\n",
    "\n",
    "class HadLinear(nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(weight/math.sqrt(BLOCK_SIZE))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = pad_to_block(input, [-1])\n",
    "        mult = input.shape[-1] // BLOCK_SIZE\n",
    "        input = input.reshape(input.shape[:-1] + (mult, BLOCK_SIZE))\n",
    "        input = hadamard_transform(input, scale=1/math.sqrt(BLOCK_SIZE))\n",
    "        input = input.reshape(input.shape[:-2] + (mult * BLOCK_SIZE,))\n",
    "        return F.linear(input, self.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.load(\"./weight.pt\").to(DEVICE)\n",
    "hessian = torch.load(\"./hessian.pt\").to(DEVICE)\n",
    "inputs = torch.load(\"./inputs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_weight = gptq(weight, 4, hessian, do_hadamard=False)\n",
    "had_weight = gptq(weight, 4, hessian, do_hadamard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7137.40625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_error = 0\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        normal_error += torch.linalg.norm(F.linear(input.to(DEVICE), weight) - F.linear(input.to(DEVICE), normal_weight)).item()\n",
    "normal_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4166.28125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "had_linear = HadLinear(had_weight)\n",
    "\n",
    "had_error = 0\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        had_error += torch.linalg.norm(F.linear(input.to(DEVICE), weight) - had_linear(input.to(DEVICE))).item()\n",
    "had_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/CompressionEntropy/./gptq/hadamard.py:85: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at ../aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  assignments = torch.bucketize(weight, boundaries[bits])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 2.70\n",
      "error 2072563.75\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./gptq\")\n",
    "\n",
    "from gptq import GPTQ\n",
    "from quant import Quantizer\n",
    "from hadamard import HadLinear\n",
    "\n",
    "layer = torch.nn.Linear(weight.shape[1], weight.shape[0], bias=False, device=DEVICE, dtype=weight.dtype)\n",
    "layer.weight.data = weight\n",
    "\n",
    "gptq_processor = GPTQ(layer)\n",
    "gptq_processor.quantizer = Quantizer()\n",
    "gptq_processor.quantizer.configure(4)\n",
    "gptq_processor.H = hessian\n",
    "gptq_processor.nsamples = 1\n",
    "\n",
    "res = gptq_processor.fasterquant(\n",
    "    groupsize=BLOCK_SIZE, clip=False, baseline=False, had=True, eden=True,\n",
    ")\n",
    "one_quant_layer = HadLinear(layer.weight.data, BLOCK_SIZE, True, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4141.0625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_quant_error = 0\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        one_quant_error += torch.linalg.norm(F.linear(input.to(DEVICE), weight) - one_quant_layer(input.to(DEVICE))).item()\n",
    "one_quant_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_quant_layer = HadLinear(layer.weight.data, BLOCK_SIZE, True, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6741.5625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_quant_error = 0\n",
    "with torch.no_grad():\n",
    "    for input in inputs:\n",
    "        double_quant_error += torch.linalg.norm(F.linear(input.to(DEVICE), weight) - double_quant_layer(input.to(DEVICE))).item()\n",
    "double_quant_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e8p2 import QUIP_SHARP_QUANTIZER_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.empty((1024, 8), device=DEVICE).normal_()\n",
    "    dequant = QUIP_SHARP_QUANTIZER_2.quantize(x, return_idx=False)\n",
    "    quip_sharp_2_mse = (x - dequant).pow(2).mean().item()\n",
    "    eden_2_mse = (x - quantize_hadamard(x, 2)).pow(2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDEN2_GRID_2 = torch.from_numpy(np.load(\"EDEN2-2.npy\")).to(torch.float32).cuda()\n",
    "EDEN2_GRID_2_NORM = torch.linalg.norm(EDEN2_GRID_2, dim=-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.empty((1024, 2), device=DEVICE).normal_()\n",
    "    idx = torch.argmax(2 * x @ EDEN2_GRID_2.T - EDEN2_GRID_2_NORM, dim=-1)\n",
    "    dequant = EDEN2_GRID_2[idx]\n",
    "    eden2_2_mse = (x - dequant).pow(2).mean().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDEN3_GRID_2 = torch.from_numpy(np.load(\"EDEN3-2.npy\")).to(torch.float32).cuda()\n",
    "EDEN3_GRID_2_NORM = torch.linalg.norm(EDEN3_GRID_2, dim=-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.empty((1024, 3), device=DEVICE).normal_()\n",
    "    idx = torch.argmax(2 * x @ EDEN3_GRID_2.T - EDEN3_GRID_2_NORM, dim=-1)\n",
    "    dequant = EDEN3_GRID_2[idx]\n",
    "    eden3_2_mse = (x - dequant).pow(2).mean().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e8p3 import QUIP_SHARP_QUANTIZER_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.empty((1024, 8), device=DEVICE).normal_()\n",
    "    dequant = QUIP_SHARP_QUANTIZER_3.quantize(x, return_idx=False)\n",
    "    quip_sharp_3_mse = (x - dequant).pow(2).mean().item()\n",
    "    eden_3_mse = (x - quantize_hadamard(x, 3)).pow(2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDEN2_GRID_3 = torch.from_numpy(np.load(\"EDEN2-3.npy\")).to(torch.float32).cuda()\n",
    "EDEN2_GRID_3_NORM = torch.linalg.norm(EDEN2_GRID_3, dim=-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.empty((1024, 2), device=DEVICE).normal_()\n",
    "    idx = torch.argmax(2 * x @ EDEN2_GRID_3.T - EDEN2_GRID_3_NORM, dim=-1)\n",
    "    dequant = EDEN2_GRID_3[idx]\n",
    "    eden2_3_mse = (x - dequant).pow(2).mean().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDEN3_GRID_3 = torch.from_numpy(np.load(\"EDEN3-3.npy\")).to(torch.float32).cuda()\n",
    "EDEN3_GRID_3_NORM = torch.linalg.norm(EDEN3_GRID_3, dim=-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.empty((1024, 3), device=DEVICE).normal_()\n",
    "    idx = torch.argmax(2 * x @ EDEN3_GRID_3.T - EDEN3_GRID_3_NORM, dim=-1)\n",
    "    dequant = EDEN3_GRID_3[idx]\n",
    "    eden3_3_mse = (x - dequant).pow(2).mean().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e8p4 import QUIP_SHARP_QUANTIZER_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.empty((1024, 8), device=DEVICE).normal_()\n",
    "    dequant = QUIP_SHARP_QUANTIZER_4.quantize(x, return_idx=False)\n",
    "    quip_sharp_4_mse = (x - dequant).pow(2).mean().item()\n",
    "    eden_4_mse = (x - quantize_hadamard(x, 4)).pow(2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDEN2_GRID_4 = torch.from_numpy(np.load(\"EDEN2-4.npy\")).to(torch.float32).cuda()\n",
    "EDEN2_GRID_4_NORM = torch.linalg.norm(EDEN2_GRID_4, dim=-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.empty((1024, 2), device=DEVICE).normal_()\n",
    "    idx = torch.argmax(2 * x @ EDEN2_GRID_4.T - EDEN2_GRID_4_NORM, dim=-1)\n",
    "    dequant = EDEN2_GRID_4[idx]\n",
    "    eden2_4_mse = (x - dequant).pow(2).mean().item()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "species = (\"2 Bit\", \"3 Bit\", \"4 Bit\")\n",
    "penguin_means = {\n",
    "    'QuIP#': (quip_sharp_2_mse, quip_sharp_3_mse, quip_sharp_4_mse),\n",
    "    'EDEN': (eden_2_mse, eden_3_mse, eden_4_mse),\n",
    "    'EDEN2': (eden2_2_mse, eden2_3_mse, eden2_4_mse),\n",
    "    'EDEN3': (eden3_2_mse, eden3_3_mse, 0),\n",
    "}\n",
    "\n",
    "x = np.arange(len(species))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, measurement in penguin_means.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    # ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Length (mm)')\n",
    "ax.set_title('Penguin attributes by species')\n",
    "ax.set_xticks(x + width, species)\n",
    "ax.legend(loc='upper left', ncols=3)\n",
    "# ax.set_ylim(0, 250)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
