{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/GPTQv2-Dev/auto_gptq/nn_modules/triton_utils/kernels.py:411: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):\n",
      "/nfs/scistore19/alistgrp/apanfero/GPTQv2-Dev/auto_gptq/nn_modules/triton_utils/kernels.py:419: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "/nfs/scistore19/alistgrp/apanfero/GPTQv2-Dev/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bbe2363d1c494aa5f6a1f67d867e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "# All linear layers will use the same quantization config\n",
    "quant_config = BaseQuantizeConfig(\n",
    "    bits=4,\n",
    "    group_size=64\n",
    ")\n",
    "\n",
    "# Load and quantize\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    quant_config,\n",
    "    torch_dtype=\"auto\", \n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gptq.datautils import get_loaders\n",
    "\n",
    "dataloader, testloader = get_loaders(\n",
    "    \"red\", seed=0, model=\"meta-llama/Meta-Llama-3.1-8B\", seqlen=8192\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - If you have not already done so, please inject the following code at the very top of your \n",
      "quantization script so the packing stage is optimized for speed. Using too many cores may reduce packing performance.\n",
      "----\n",
      "import os\n",
      "import math\n",
      "max_threads = str(min(8, os.cpu_count()))\n",
      "os.environ['OMP_NUM_THREADS'] = max_threads\n",
      "os.environ['OPENBLAS_NUM_THREADS'] = max_threads\n",
      "os.environ['MKL_NUM_THREADS'] = max_threads\n",
      "os.environ['VECLIB_MAXIMUM_THREADS'] = max_threads\n",
      "os.environ['NUMEXPR_NUM_THREADS'] = max_threads\n",
      "os.environ['NUMEXPR_MAX_THREADS'] = max_threads\n",
      "----\n",
      "\n",
      "INFO - Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "INFO - Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "INFO - Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "INFO - Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "INFO - Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "INFO - Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "INFO - Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "INFO - Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "INFO - Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "INFO - Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "INFO - Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "INFO - Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "INFO - Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "INFO - Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "INFO - Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "INFO - Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "INFO - Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "INFO - Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "INFO - Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "INFO - Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "INFO - Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "INFO - Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "INFO - Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "INFO - Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "INFO - Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "INFO - Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "INFO - Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "INFO - Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "INFO - Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "INFO - Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "INFO - Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "INFO - Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "INFO - Packing model...\n",
      "Packing model.layers.31.mlp.down_proj...: 100%|██████████| 224/224 [03:49<00:00,  1.03s/it]   \n",
      "INFO - Model packed.\n"
     ]
    }
   ],
   "source": [
    "model.quantize([{\"input_ids\": data, \"attention_mask\": torch.ones_like(data)} for data in dataloader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_shots(model, task_list = ('arc_easy',), num_fewshots=1):\n",
    "    import lm_eval\n",
    "\n",
    "    lm_eval_model = lm_eval.models.huggingface.HFLM(\n",
    "        pretrained=model,\n",
    "    )\n",
    "\n",
    "    tasks = lm_eval.tasks.get_task_dict(task_list)\n",
    "    if num_fewshots != 1:\n",
    "        # TODO: make fewshots properly\n",
    "        for task_name in tasks:\n",
    "            task = tasks[task_name]\n",
    "            if isinstance(task, tuple):\n",
    "                task = task[1]\n",
    "            if task is None:\n",
    "                continue\n",
    "            task.config.num_fewshot = num_fewshots\n",
    "\n",
    "    results = lm_eval.evaluator.evaluate(\n",
    "        lm=lm_eval_model,\n",
    "        task_dict=tasks,\n",
    "    )\n",
    "\n",
    "    result_dict = {task_name: task_result['acc,none'] for task_name, task_result in results['results'].items()}\n",
    "    result_err_dict = {f'{task_name}_err': task_result['acc_stderr,none'] for task_name, task_result in\n",
    "                       results['results'].items()}\n",
    "    result_dict = dict(list(result_dict.items()) + list(result_err_dict.items()))\n",
    "\n",
    "    if num_fewshots != 1:\n",
    "        result_dict = {f'{task_name}@{num_fewshots}': acc for task_name, acc in result_dict.items()}\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def llama_eval(model, dataloader):\n",
    "    print('Evaluating ...')\n",
    "\n",
    "    nsamples = len(dataloader) \n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    model.model.embed_tokens = model.model.embed_tokens\n",
    "    model.model.rotary_emb = model.model.rotary_emb\n",
    "    layers[0] = layers[0]\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = []\n",
    "    attention_masks = []\n",
    "    position_ids = []\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps.append(inp)\n",
    "            attention_masks.append(kwargs['attention_mask'])\n",
    "            position_ids.append(kwargs['position_ids'])\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch.to(\"cuda\"))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    layers[0] = layers[0]\n",
    "    model.model.embed_tokens = model.model.embed_tokens\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for i in trange(len(layers), desc=f\"Evaluating layer-by-layer...\"):\n",
    "        layer = layers[i]\n",
    "        for j in range(nsamples):\n",
    "            inps[j] = layer(inps[j], attention_mask=attention_masks[j], position_ids=position_ids[j])[0]\n",
    "        layers[i] = layer\n",
    "        del layer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if model.model.norm is not None:\n",
    "        model.model.norm = model.model.norm\n",
    "    model.lm_head = model.lm_head\n",
    "\n",
    "    nlls = []\n",
    "    for i in range(nsamples):\n",
    "        hidden_states = inps[i]\n",
    "        if model.model.norm is not None:\n",
    "            hidden_states = model.model.norm(hidden_states)\n",
    "        lm_logits = model.lm_head(hidden_states)\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = (dataloader[i].to(\"cuda\"))[:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * 8192\n",
    "        nlls.append(neg_log_likelihood)\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * 8192))\n",
    "    print(ppl.item())\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    \n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c733896f97b478eb3a8fad45b81642a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating layer-by-layer...:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0052490234375\n"
     ]
    }
   ],
   "source": [
    "from gptq.datautils import get_loaders\n",
    "\n",
    "datasets = ['wikitext2'] \n",
    "for dataset in datasets:\n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=0, model=\"meta-llama/Meta-Llama-3.1-8B\", seqlen=8192\n",
    "    )\n",
    "    ppl = llama_eval(model.model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14:13:03:43,829 WARNING  [huggingface.py:118] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-10-14:13:03:45,151 WARNING  [huggingface.py:337] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for winogrande contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/winogrande\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hellaswag contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hellaswag\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-10-14:13:04:13,406 INFO     [task.py:386] Building contexts for arc_challenge on rank 0...\n",
      "100%|██████████| 1172/1172 [00:00<00:00, 1421.84it/s]\n",
      "2024-10-14:13:04:14,284 INFO     [task.py:386] Building contexts for winogrande on rank 0...\n",
      "100%|██████████| 1267/1267 [00:00<00:00, 91620.69it/s]\n",
      "2024-10-14:13:04:14,334 INFO     [task.py:386] Building contexts for hellaswag on rank 0...\n",
      "100%|██████████| 10042/10042 [00:03<00:00, 3064.45it/s]\n",
      "2024-10-14:13:04:18,455 INFO     [task.py:386] Building contexts for piqa on rank 0...\n",
      "100%|██████████| 1838/1838 [00:01<00:00, 1359.60it/s]\n",
      "2024-10-14:13:04:19,862 INFO     [task.py:386] Building contexts for arc_easy on rank 0...\n",
      "100%|██████████| 2376/2376 [00:01<00:00, 1433.03it/s]\n",
      "2024-10-14:13:04:21,619 INFO     [evaluator.py:359] Running loglikelihood requests\n"
     ]
    }
   ],
   "source": [
    "results = get_zero_shots(\n",
    "    model,\n",
    "    task_list=(\"winogrande\",\"arc_easy\",\"piqa\",\"hellaswag\",\"winogrande\",\"arc_challenge\"),\n",
    "    num_fewshots=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hqq 4 64\n",
    "```json\n",
    "{'mmlu@5': 0.6378008830650904,\n",
    " 'mmlu_humanities@5': 0.5874601487778959,\n",
    " 'mmlu_formal_logic@5': 0.42857142857142855,\n",
    " 'mmlu_high_school_european_history@5': 0.7575757575757576,\n",
    " 'mmlu_high_school_us_history@5': 0.8235294117647058,\n",
    " 'mmlu_high_school_world_history@5': 0.810126582278481,\n",
    " 'mmlu_international_law@5': 0.8099173553719008,\n",
    " 'mmlu_jurisprudence@5': 0.7407407407407407,\n",
    " 'mmlu_logical_fallacies@5': 0.7484662576687117,\n",
    " 'mmlu_moral_disputes@5': 0.7109826589595376,\n",
    " 'mmlu_moral_scenarios@5': 0.37988826815642457,\n",
    " 'mmlu_philosophy@5': 0.7234726688102894,\n",
    " 'mmlu_prehistory@5': 0.7314814814814815,\n",
    " 'mmlu_professional_law@5': 0.4804432855280313,\n",
    " 'mmlu_world_religions@5': 0.8187134502923976,\n",
    " 'mmlu_other@5': 0.7103315094946894,\n",
    " 'mmlu_business_ethics@5': 0.61,\n",
    " 'mmlu_clinical_knowledge@5': 0.7358490566037735,\n",
    " 'mmlu_college_medicine@5': 0.6473988439306358,\n",
    " 'mmlu_global_facts@5': 0.37,\n",
    " 'mmlu_human_aging@5': 0.6771300448430493,\n",
    " 'mmlu_management@5': 0.7961165048543689,\n",
    " 'mmlu_marketing@5': 0.8846153846153846,\n",
    " 'mmlu_medical_genetics@5': 0.75,\n",
    " 'mmlu_miscellaneous@5': 0.8212005108556832,\n",
    " 'mmlu_nutrition@5': 0.7287581699346405,\n",
    " 'mmlu_professional_accounting@5': 0.4858156028368794,\n",
    " 'mmlu_professional_medicine@5': 0.7022058823529411,\n",
    " 'mmlu_virology@5': 0.5602409638554217,\n",
    " 'mmlu_social_sciences@5': 0.7419564510887228,\n",
    " 'mmlu_econometrics@5': 0.5,\n",
    " 'mmlu_high_school_geography@5': 0.8131313131313131,\n",
    " 'mmlu_high_school_government_and_politics@5': 0.8290155440414507,\n",
    " 'mmlu_high_school_macroeconomics@5': 0.6307692307692307,\n",
    " 'mmlu_high_school_microeconomics@5': 0.6974789915966386,\n",
    " 'mmlu_high_school_psychology@5': 0.8422018348623853,\n",
    " 'mmlu_human_sexuality@5': 0.7557251908396947,\n",
    " 'mmlu_professional_psychology@5': 0.6911764705882353,\n",
    " 'mmlu_public_relations@5': 0.7181818181818181,\n",
    " 'mmlu_security_studies@5': 0.726530612244898,\n",
    " 'mmlu_sociology@5': 0.8507462686567164,\n",
    " 'mmlu_us_foreign_policy@5': 0.84,\n",
    " 'mmlu_stem@5': 0.5398033618775769,\n",
    " 'mmlu_abstract_algebra@5': 0.31,\n",
    " 'mmlu_anatomy@5': 0.5925925925925926,\n",
    " 'mmlu_astronomy@5': 0.6842105263157895,\n",
    " 'mmlu_college_biology@5': 0.7638888888888888,\n",
    " 'mmlu_college_chemistry@5': 0.44,\n",
    " 'mmlu_college_computer_science@5': 0.53,\n",
    " 'mmlu_college_mathematics@5': 0.33,\n",
    " 'mmlu_college_physics@5': 0.45098039215686275,\n",
    " 'mmlu_computer_security@5': 0.77,\n",
    " 'mmlu_conceptual_physics@5': 0.5829787234042553,\n",
    " 'mmlu_electrical_engineering@5': 0.6344827586206897,\n",
    " 'mmlu_elementary_mathematics@5': 0.3968253968253968,\n",
    " 'mmlu_high_school_biology@5': 0.7645161290322581,\n",
    " 'mmlu_high_school_chemistry@5': 0.5369458128078818,\n",
    " 'mmlu_high_school_computer_science@5': 0.62,\n",
    " 'mmlu_high_school_mathematics@5': 0.40370370370370373,\n",
    " 'mmlu_high_school_physics@5': 0.4304635761589404,\n",
    " 'mmlu_high_school_statistics@5': 0.5462962962962963,\n",
    " 'mmlu_machine_learning@5': 0.4017857142857143,\n",
    " 'mmlu_err@5': 0.003835221836153209,\n",
    " 'mmlu_humanities_err@5': 0.006775368801783291,\n",
    " 'mmlu_formal_logic_err@5': 0.0442626668137991,\n",
    " 'mmlu_high_school_european_history_err@5': 0.03346409881055953,\n",
    " 'mmlu_high_school_us_history_err@5': 0.02675640153807897,\n",
    " 'mmlu_high_school_world_history_err@5': 0.02553010046023351,\n",
    " 'mmlu_international_law_err@5': 0.03581796951709282,\n",
    " 'mmlu_jurisprudence_err@5': 0.04236511258094633,\n",
    " 'mmlu_logical_fallacies_err@5': 0.03408997886857529,\n",
    " 'mmlu_moral_disputes_err@5': 0.024405173935783234,\n",
    " 'mmlu_moral_scenarios_err@5': 0.016232826818678495,\n",
    " 'mmlu_philosophy_err@5': 0.02540383297817961,\n",
    " 'mmlu_prehistory_err@5': 0.024659685185967284,\n",
    " 'mmlu_professional_law_err@5': 0.012760464028289299,\n",
    " 'mmlu_world_religions_err@5': 0.029547741687640038,\n",
    " 'mmlu_other_err@5': 0.007833424946677572,\n",
    " 'mmlu_business_ethics_err@5': 0.04902071300001974,\n",
    " 'mmlu_clinical_knowledge_err@5': 0.027134291628741727,\n",
    " 'mmlu_college_medicine_err@5': 0.036430371689585475,\n",
    " 'mmlu_global_facts_err@5': 0.04852365870939098,\n",
    " 'mmlu_human_aging_err@5': 0.031381476375754995,\n",
    " 'mmlu_management_err@5': 0.03989139859531769,\n",
    " 'mmlu_marketing_err@5': 0.02093019318517933,\n",
    " 'mmlu_medical_genetics_err@5': 0.04351941398892446,\n",
    " 'mmlu_miscellaneous_err@5': 0.013702643715368983,\n",
    " 'mmlu_nutrition_err@5': 0.025457756696667864,\n",
    " 'mmlu_professional_accounting_err@5': 0.02981549448368206,\n",
    " 'mmlu_professional_medicine_err@5': 0.027778298701545447,\n",
    " 'mmlu_virology_err@5': 0.03864139923699121,\n",
    " 'mmlu_social_sciences_err@5': 0.007734190961291264,\n",
    " 'mmlu_econometrics_err@5': 0.047036043419179864,\n",
    " 'mmlu_high_school_geography_err@5': 0.02777253333421898,\n",
    " 'mmlu_high_school_government_and_politics_err@5': 0.027171213683164545,\n",
    " 'mmlu_high_school_macroeconomics_err@5': 0.024468615241478916,\n",
    " 'mmlu_high_school_microeconomics_err@5': 0.029837962388291932,\n",
    " 'mmlu_high_school_psychology_err@5': 0.015630022970092455,\n",
    " 'mmlu_human_sexuality_err@5': 0.037683359597287434,\n",
    " 'mmlu_professional_psychology_err@5': 0.018690850273595284,\n",
    " 'mmlu_public_relations_err@5': 0.04309118709946459,\n",
    " 'mmlu_security_studies_err@5': 0.028535560337128448,\n",
    " 'mmlu_sociology_err@5': 0.025196929874827072,\n",
    " 'mmlu_us_foreign_policy_err@5': 0.03684529491774707,\n",
    " 'mmlu_stem_err@5': 0.008541772519633976,\n",
    " 'mmlu_abstract_algebra_err@5': 0.04648231987117316,\n",
    " 'mmlu_anatomy_err@5': 0.04244633238353228,\n",
    " 'mmlu_astronomy_err@5': 0.03782728980865469,\n",
    " 'mmlu_college_biology_err@5': 0.03551446610810826,\n",
    " 'mmlu_college_chemistry_err@5': 0.04988876515698589,\n",
    " 'mmlu_college_computer_science_err@5': 0.050161355804659205,\n",
    " 'mmlu_college_mathematics_err@5': 0.04725815626252606,\n",
    " 'mmlu_college_physics_err@5': 0.04951218252396262,\n",
    " 'mmlu_computer_security_err@5': 0.04229525846816505,\n",
    " 'mmlu_conceptual_physics_err@5': 0.03223276266711712,\n",
    " 'mmlu_electrical_engineering_err@5': 0.04013124195424385,\n",
    " 'mmlu_elementary_mathematics_err@5': 0.025197101074246494,\n",
    " 'mmlu_high_school_biology_err@5': 0.024137632429337703,\n",
    " 'mmlu_high_school_chemistry_err@5': 0.035083705204426656,\n",
    " 'mmlu_high_school_computer_science_err@5': 0.048783173121456316,\n",
    " 'mmlu_high_school_mathematics_err@5': 0.02991481234222763,\n",
    " 'mmlu_high_school_physics_err@5': 0.04042809961395634,\n",
    " 'mmlu_high_school_statistics_err@5': 0.033953227263757976,\n",
    " 'mmlu_machine_learning_err@5': 0.04653333146973647}\n",
    "```\n",
    "\n",
    "hqq 8 64\n",
    "```json\n",
    "{'mmlu@5': 0.654037886340977, 'mmlu_humanities@5': 0.6006376195536663, 'mmlu_formal_logic@5': 0.47619047619047616, 'mmlu_high_school_european_history@5': 0.7818181818181819, 'mmlu_high_school_us_history@5': 0.8235294117647058, 'mmlu_high_school_world_history@5': 0.8227848101265823, 'mmlu_international_law@5': 0.8264462809917356, 'mmlu_jurisprudence@5': 0.7407407407407407, 'mmlu_logical_fallacies@5': 0.7423312883435583, 'mmlu_moral_disputes@5': 0.7225433526011561, 'mmlu_moral_scenarios@5': 0.4122905027932961, 'mmlu_philosophy@5': 0.7266881028938906, 'mmlu_prehistory@5': 0.7253086419753086, 'mmlu_professional_law@5': 0.4921773142112125, 'mmlu_world_religions@5': 0.8070175438596491, 'mmlu_other@5': 0.7206308336015449, 'mmlu_business_ethics@5': 0.65, 'mmlu_clinical_knowledge@5': 0.7584905660377359, 'mmlu_college_medicine@5': 0.6473988439306358, 'mmlu_global_facts@5': 0.33, 'mmlu_human_aging@5': 0.695067264573991, 'mmlu_management@5': 0.8446601941747572, 'mmlu_marketing@5': 0.8589743589743589, 'mmlu_medical_genetics@5': 0.83, 'mmlu_miscellaneous@5': 0.80970625798212, 'mmlu_nutrition@5': 0.7973856209150327, 'mmlu_professional_accounting@5': 0.5, 'mmlu_professional_medicine@5': 0.6911764705882353, 'mmlu_virology@5': 0.572289156626506, 'mmlu_social_sciences@5': 0.7630809229769255, 'mmlu_econometrics@5': 0.49122807017543857, 'mmlu_high_school_geography@5': 0.8080808080808081, 'mmlu_high_school_government_and_politics@5': 0.8963730569948186, 'mmlu_high_school_macroeconomics@5': 0.6487179487179487, 'mmlu_high_school_microeconomics@5': 0.7352941176470589, 'mmlu_high_school_psychology@5': 0.8495412844036697, 'mmlu_human_sexuality@5': 0.7709923664122137, 'mmlu_professional_psychology@5': 0.7238562091503268, 'mmlu_public_relations@5': 0.7090909090909091, 'mmlu_security_studies@5': 0.7346938775510204, 'mmlu_sociology@5': 0.8805970149253731, 'mmlu_us_foreign_policy@5': 0.89, 'mmlu_stem@5': 0.5616872819536949, 'mmlu_abstract_algebra@5': 0.29, 'mmlu_anatomy@5': 0.6148148148148148, 'mmlu_astronomy@5': 0.7236842105263158, 'mmlu_college_biology@5': 0.7847222222222222, 'mmlu_college_chemistry@5': 0.46, 'mmlu_college_computer_science@5': 0.49, 'mmlu_college_mathematics@5': 0.33, 'mmlu_college_physics@5': 0.5, 'mmlu_computer_security@5': 0.85, 'mmlu_conceptual_physics@5': 0.6127659574468085, 'mmlu_electrical_engineering@5': 0.6413793103448275, 'mmlu_elementary_mathematics@5': 0.42857142857142855, 'mmlu_high_school_biology@5': 0.7870967741935484, 'mmlu_high_school_chemistry@5': 0.5320197044334976, 'mmlu_high_school_computer_science@5': 0.68, 'mmlu_high_school_mathematics@5': 0.42962962962962964, 'mmlu_high_school_physics@5': 0.44370860927152317, 'mmlu_high_school_statistics@5': 0.5555555555555556, 'mmlu_machine_learning@5': 0.44642857142857145, 'mmlu_err@5': 0.0037947049308790343, 'mmlu_humanities_err@5': 0.00678283136290411, 'mmlu_formal_logic_err@5': 0.04467062628403273, 'mmlu_high_school_european_history_err@5': 0.03225078108306289, 'mmlu_high_school_us_history_err@5': 0.026756401538078955, 'mmlu_high_school_world_history_err@5': 0.02485636418450323, 'mmlu_international_law_err@5': 0.0345727283691767, 'mmlu_jurisprudence_err@5': 0.042365112580946336, 'mmlu_logical_fallacies_err@5': 0.03436150827846917, 'mmlu_moral_disputes_err@5': 0.024105712607754307, 'mmlu_moral_scenarios_err@5': 0.01646320023811451, 'mmlu_philosophy_err@5': 0.02531176597542611, 'mmlu_prehistory_err@5': 0.024836057868294677, 'mmlu_professional_law_err@5': 0.0127686730761119, 'mmlu_world_religions_err@5': 0.030267457554898458, 'mmlu_other_err@5': 0.007736708380444025, 'mmlu_business_ethics_err@5': 0.047937248544110196, 'mmlu_clinical_knowledge_err@5': 0.026341480371118362, 'mmlu_college_medicine_err@5': 0.036430371689585475, 'mmlu_global_facts_err@5': 0.047258156262526045, 'mmlu_human_aging_err@5': 0.030898610882477515, 'mmlu_management_err@5': 0.03586594738573974, 'mmlu_marketing_err@5': 0.022801382534597542, 'mmlu_medical_genetics_err@5': 0.0377525168068637, 'mmlu_miscellaneous_err@5': 0.014036945850381385, 'mmlu_nutrition_err@5': 0.023015446877985655, 'mmlu_professional_accounting_err@5': 0.029827499313594685, 'mmlu_professional_medicine_err@5': 0.028064998167040094, 'mmlu_virology_err@5': 0.038515976837185335, 'mmlu_social_sciences_err@5': 0.007488141850953062, 'mmlu_econometrics_err@5': 0.04702880432049615, 'mmlu_high_school_geography_err@5': 0.028057791672989017, 'mmlu_high_school_government_and_politics_err@5': 0.021995311963644237, 'mmlu_high_school_macroeconomics_err@5': 0.024203665177902803, 'mmlu_high_school_microeconomics_err@5': 0.028657491285071977, 'mmlu_high_school_psychology_err@5': 0.01532856\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
