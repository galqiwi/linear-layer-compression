{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, HqqConfig, GPTQConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "\u001b[33mWarning: Quantized meta-data is deprecated and will be removed. It is not supported for quantized model serialization.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1508e11f49c242f3a036dc9c6054d262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWarning: Quantizing zeros/scales is deprecated. This setting will be ignored.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, HqqConfig\n",
    "\n",
    "# All linear layers will use the same quantization config\n",
    "quant_config = HqqConfig(nbits=3, group_size=64)\n",
    "\n",
    "# Load and quantize\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B\", \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"cuda\", \n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_shots(model, task_list = ('arc_easy',), num_fewshots=1):\n",
    "    import lm_eval\n",
    "\n",
    "    lm_eval_model = lm_eval.models.huggingface.HFLM(\n",
    "        pretrained=model,\n",
    "    )\n",
    "\n",
    "    tasks = lm_eval.tasks.get_task_dict(task_list)\n",
    "    if num_fewshots != 1:\n",
    "        # TODO: make fewshots properly\n",
    "        for task_name in tasks:\n",
    "            task = tasks[task_name]\n",
    "            if isinstance(task, tuple):\n",
    "                task = task[1]\n",
    "            if task is None:\n",
    "                continue\n",
    "            task.config.num_fewshot = num_fewshots\n",
    "\n",
    "    results = lm_eval.evaluator.evaluate(\n",
    "        lm=lm_eval_model,\n",
    "        task_dict=tasks,\n",
    "    )\n",
    "\n",
    "    result_dict = {task_name: task_result['acc,none'] for task_name, task_result in results['results'].items()}\n",
    "    result_err_dict = {f'{task_name}_err': task_result['acc_stderr,none'] for task_name, task_result in\n",
    "                       results['results'].items()}\n",
    "    result_dict = dict(list(result_dict.items()) + list(result_err_dict.items()))\n",
    "\n",
    "    if num_fewshots != 1:\n",
    "        result_dict = {f'{task_name}@{num_fewshots}': acc for task_name, acc in result_dict.items()}\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def llama_eval(model, dataloader, dev):\n",
    "    print('Evaluating ...')\n",
    "\n",
    "    nsamples = len(dataloader) \n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n",
    "    model.model.rotary_emb = model.model.rotary_emb.to(dev)\n",
    "    layers[0] = layers[0].to(dev)\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = []\n",
    "    attention_masks = []\n",
    "    position_ids = []\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps.append(inp)\n",
    "            attention_masks.append(kwargs['attention_mask'])\n",
    "            position_ids.append(kwargs['position_ids'])\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch.to(dev))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    layers[0] = layers[0].cpu()\n",
    "    model.model.embed_tokens = model.model.embed_tokens.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for i in trange(len(layers), desc=f\"Evaluating layer-by-layer...\"):\n",
    "        layer = layers[i].to(dev)\n",
    "        for j in range(nsamples):\n",
    "            inps[j] = layer(inps[j], attention_mask=attention_masks[j], position_ids=position_ids[j])[0]\n",
    "        layers[i] = layer.cpu()\n",
    "        del layer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if model.model.norm is not None:\n",
    "        model.model.norm = model.model.norm.to(dev)\n",
    "    model.lm_head = model.lm_head.to(dev)\n",
    "\n",
    "    nlls = []\n",
    "    for i in range(nsamples):\n",
    "        hidden_states = inps[i]\n",
    "        if model.model.norm is not None:\n",
    "            hidden_states = model.model.norm(hidden_states)\n",
    "        lm_logits = model.lm_head(hidden_states)\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = (dataloader[i].to(dev))[:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * 8192\n",
    "        nlls.append(neg_log_likelihood)\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * 8192))\n",
    "    print(ppl.item())\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    \n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptq.datautils import get_loaders\n",
    "\n",
    "datasets = ['wikitext2'] \n",
    "for dataset in datasets:\n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=0, model=\"meta-llama/Meta-Llama-3.1-8B\", seqlen=8192\n",
    "    )\n",
    "    ppl = llama_eval(model, testloader, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29:19:51:49,767 WARNING  [huggingface.py:118] `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.\n",
      "2024-09-29:19:51:50,785 WARNING  [huggingface.py:337] Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration\n",
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "2024-09-29:19:54:00,699 INFO     [task.py:386] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|██████████| 346/346 [00:03<00:00, 87.77it/s]\n",
      "2024-09-29:19:54:04,657 INFO     [task.py:386] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|██████████| 171/171 [00:01<00:00, 88.29it/s]\n",
      "2024-09-29:19:54:06,604 INFO     [task.py:386] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|██████████| 165/165 [00:01<00:00, 88.31it/s]\n",
      "2024-09-29:19:54:08,483 INFO     [task.py:386] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|██████████| 895/895 [00:10<00:00, 88.82it/s]\n",
      "2024-09-29:19:54:18,593 INFO     [task.py:386] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|██████████| 324/324 [00:03<00:00, 88.80it/s]\n",
      "2024-09-29:19:54:22,256 INFO     [task.py:386] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|██████████| 126/126 [00:01<00:00, 89.11it/s]\n",
      "2024-09-29:19:54:23,678 INFO     [task.py:386] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|██████████| 237/237 [00:02<00:00, 88.73it/s]\n",
      "2024-09-29:19:54:26,360 INFO     [task.py:386] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|██████████| 121/121 [00:01<00:00, 89.00it/s]\n",
      "2024-09-29:19:54:27,727 INFO     [task.py:386] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|██████████| 163/163 [00:01<00:00, 88.96it/s]\n",
      "2024-09-29:19:54:29,569 INFO     [task.py:386] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|██████████| 108/108 [00:01<00:00, 88.05it/s]\n",
      "2024-09-29:19:54:30,802 INFO     [task.py:386] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|██████████| 204/204 [00:02<00:00, 88.03it/s]\n",
      "2024-09-29:19:54:33,130 INFO     [task.py:386] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|██████████| 1534/1534 [00:17<00:00, 88.01it/s]\n",
      "2024-09-29:19:54:50,617 INFO     [task.py:386] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|██████████| 311/311 [00:03<00:00, 88.96it/s]\n",
      "2024-09-29:19:54:54,127 INFO     [task.py:386] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|██████████| 545/545 [00:06<00:00, 88.31it/s]\n",
      "2024-09-29:19:55:00,320 INFO     [task.py:386] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|██████████| 193/193 [00:02<00:00, 88.10it/s]\n",
      "2024-09-29:19:55:02,521 INFO     [task.py:386] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|██████████| 198/198 [00:02<00:00, 77.78it/s]\n",
      "2024-09-29:19:55:05,076 INFO     [task.py:386] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 87.41it/s]\n",
      "2024-09-29:19:55:06,227 INFO     [task.py:386] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|██████████| 110/110 [00:01<00:00, 88.05it/s]\n",
      "2024-09-29:19:55:07,482 INFO     [task.py:386] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|██████████| 114/114 [00:01<00:00, 87.17it/s]\n",
      "2024-09-29:19:55:08,797 INFO     [task.py:386] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|██████████| 201/201 [00:02<00:00, 88.44it/s]\n",
      "2024-09-29:19:55:11,080 INFO     [task.py:386] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|██████████| 390/390 [00:04<00:00, 88.84it/s]\n",
      "2024-09-29:19:55:15,486 INFO     [task.py:386] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|██████████| 612/612 [00:06<00:00, 88.61it/s]\n",
      "2024-09-29:19:55:22,416 INFO     [task.py:386] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|██████████| 131/131 [00:01<00:00, 88.71it/s]\n",
      "2024-09-29:19:55:23,900 INFO     [task.py:386] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|██████████| 238/238 [00:02<00:00, 88.62it/s]\n",
      "2024-09-29:19:55:26,597 INFO     [task.py:386] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|██████████| 245/245 [00:02<00:00, 88.93it/s]\n",
      "2024-09-29:19:55:29,364 INFO     [task.py:386] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|██████████| 223/223 [00:02<00:00, 88.96it/s]\n",
      "2024-09-29:19:55:31,880 INFO     [task.py:386] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.80it/s]\n",
      "2024-09-29:19:55:33,013 INFO     [task.py:386] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|██████████| 265/265 [00:02<00:00, 88.80it/s]\n",
      "2024-09-29:19:55:36,009 INFO     [task.py:386] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|██████████| 783/783 [00:08<00:00, 88.53it/s]\n",
      "2024-09-29:19:55:44,882 INFO     [task.py:386] Building contexts for mmlu_management on rank 0...\n",
      "100%|██████████| 103/103 [00:01<00:00, 88.14it/s]\n",
      "2024-09-29:19:55:46,058 INFO     [task.py:386] Building contexts for mmlu_virology on rank 0...\n",
      "100%|██████████| 166/166 [00:01<00:00, 88.32it/s]\n",
      "2024-09-29:19:55:47,946 INFO     [task.py:386] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|██████████| 272/272 [00:03<00:00, 88.47it/s]\n",
      "2024-09-29:19:55:51,033 INFO     [task.py:386] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.59it/s]\n",
      "2024-09-29:19:55:52,168 INFO     [task.py:386] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|██████████| 282/282 [00:03<00:00, 88.41it/s]\n",
      "2024-09-29:19:55:55,369 INFO     [task.py:386] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.15it/s]\n",
      "2024-09-29:19:55:56,511 INFO     [task.py:386] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|██████████| 173/173 [00:01<00:00, 88.36it/s]\n",
      "2024-09-29:19:55:58,477 INFO     [task.py:386] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|██████████| 234/234 [00:02<00:00, 88.89it/s]\n",
      "2024-09-29:19:56:01,120 INFO     [task.py:386] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|██████████| 306/306 [00:03<00:00, 88.97it/s]\n",
      "2024-09-29:19:56:04,572 INFO     [task.py:386] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|██████████| 102/102 [00:01<00:00, 87.97it/s]\n",
      "2024-09-29:19:56:05,738 INFO     [task.py:386] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 89.00it/s]\n",
      "2024-09-29:19:56:06,868 INFO     [task.py:386] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.22it/s]\n",
      "2024-09-29:19:56:08,008 INFO     [task.py:386] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 89.55it/s]\n",
      "2024-09-29:19:56:09,132 INFO     [task.py:386] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|██████████| 144/144 [00:01<00:00, 89.21it/s]\n",
      "2024-09-29:19:56:10,754 INFO     [task.py:386] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.51it/s]\n",
      "2024-09-29:19:56:11,890 INFO     [task.py:386] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|██████████| 151/151 [00:01<00:00, 88.88it/s]\n",
      "2024-09-29:19:56:13,596 INFO     [task.py:386] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 88.73it/s]\n",
      "2024-09-29:19:56:14,730 INFO     [task.py:386] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|██████████| 270/270 [00:03<00:00, 89.00it/s]\n",
      "2024-09-29:19:56:17,775 INFO     [task.py:386] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|██████████| 135/135 [00:01<00:00, 88.87it/s]\n",
      "2024-09-29:19:56:19,302 INFO     [task.py:386] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|██████████| 310/310 [00:03<00:00, 88.97it/s]\n",
      "2024-09-29:19:56:22,799 INFO     [task.py:386] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|██████████| 235/235 [00:02<00:00, 88.92it/s]\n",
      "2024-09-29:19:56:25,456 INFO     [task.py:386] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|██████████| 378/378 [00:04<00:00, 88.99it/s]\n",
      "2024-09-29:19:56:29,718 INFO     [task.py:386] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|██████████| 216/216 [00:02<00:00, 89.13it/s]\n",
      "2024-09-29:19:56:32,152 INFO     [task.py:386] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|██████████| 152/152 [00:01<00:00, 88.86it/s]\n",
      "2024-09-29:19:56:33,871 INFO     [task.py:386] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|██████████| 145/145 [00:01<00:00, 89.01it/s]\n",
      "2024-09-29:19:56:35,508 INFO     [task.py:386] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|██████████| 100/100 [00:01<00:00, 67.23it/s]\n",
      "2024-09-29:19:56:37,001 INFO     [task.py:386] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|██████████| 112/112 [00:01<00:00, 88.93it/s]\n",
      "2024-09-29:19:56:38,268 INFO     [task.py:386] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|██████████| 203/203 [00:02<00:00, 88.98it/s]\n",
      "2024-09-29:19:56:40,559 INFO     [evaluator.py:359] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|          | 0/56168 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "Running loglikelihood requests: 100%|██████████| 56168/56168 [43:51<00:00, 21.34it/s] \n"
     ]
    }
   ],
   "source": [
    "results = get_zero_shots(\n",
    "    model,\n",
    "    task_list=(\"mmlu\",),\n",
    "    num_fewshots=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hqq 4 64\n",
    "```json\n",
    "{'mmlu@5': 0.6378008830650904,\n",
    " 'mmlu_humanities@5': 0.5874601487778959,\n",
    " 'mmlu_formal_logic@5': 0.42857142857142855,\n",
    " 'mmlu_high_school_european_history@5': 0.7575757575757576,\n",
    " 'mmlu_high_school_us_history@5': 0.8235294117647058,\n",
    " 'mmlu_high_school_world_history@5': 0.810126582278481,\n",
    " 'mmlu_international_law@5': 0.8099173553719008,\n",
    " 'mmlu_jurisprudence@5': 0.7407407407407407,\n",
    " 'mmlu_logical_fallacies@5': 0.7484662576687117,\n",
    " 'mmlu_moral_disputes@5': 0.7109826589595376,\n",
    " 'mmlu_moral_scenarios@5': 0.37988826815642457,\n",
    " 'mmlu_philosophy@5': 0.7234726688102894,\n",
    " 'mmlu_prehistory@5': 0.7314814814814815,\n",
    " 'mmlu_professional_law@5': 0.4804432855280313,\n",
    " 'mmlu_world_religions@5': 0.8187134502923976,\n",
    " 'mmlu_other@5': 0.7103315094946894,\n",
    " 'mmlu_business_ethics@5': 0.61,\n",
    " 'mmlu_clinical_knowledge@5': 0.7358490566037735,\n",
    " 'mmlu_college_medicine@5': 0.6473988439306358,\n",
    " 'mmlu_global_facts@5': 0.37,\n",
    " 'mmlu_human_aging@5': 0.6771300448430493,\n",
    " 'mmlu_management@5': 0.7961165048543689,\n",
    " 'mmlu_marketing@5': 0.8846153846153846,\n",
    " 'mmlu_medical_genetics@5': 0.75,\n",
    " 'mmlu_miscellaneous@5': 0.8212005108556832,\n",
    " 'mmlu_nutrition@5': 0.7287581699346405,\n",
    " 'mmlu_professional_accounting@5': 0.4858156028368794,\n",
    " 'mmlu_professional_medicine@5': 0.7022058823529411,\n",
    " 'mmlu_virology@5': 0.5602409638554217,\n",
    " 'mmlu_social_sciences@5': 0.7419564510887228,\n",
    " 'mmlu_econometrics@5': 0.5,\n",
    " 'mmlu_high_school_geography@5': 0.8131313131313131,\n",
    " 'mmlu_high_school_government_and_politics@5': 0.8290155440414507,\n",
    " 'mmlu_high_school_macroeconomics@5': 0.6307692307692307,\n",
    " 'mmlu_high_school_microeconomics@5': 0.6974789915966386,\n",
    " 'mmlu_high_school_psychology@5': 0.8422018348623853,\n",
    " 'mmlu_human_sexuality@5': 0.7557251908396947,\n",
    " 'mmlu_professional_psychology@5': 0.6911764705882353,\n",
    " 'mmlu_public_relations@5': 0.7181818181818181,\n",
    " 'mmlu_security_studies@5': 0.726530612244898,\n",
    " 'mmlu_sociology@5': 0.8507462686567164,\n",
    " 'mmlu_us_foreign_policy@5': 0.84,\n",
    " 'mmlu_stem@5': 0.5398033618775769,\n",
    " 'mmlu_abstract_algebra@5': 0.31,\n",
    " 'mmlu_anatomy@5': 0.5925925925925926,\n",
    " 'mmlu_astronomy@5': 0.6842105263157895,\n",
    " 'mmlu_college_biology@5': 0.7638888888888888,\n",
    " 'mmlu_college_chemistry@5': 0.44,\n",
    " 'mmlu_college_computer_science@5': 0.53,\n",
    " 'mmlu_college_mathematics@5': 0.33,\n",
    " 'mmlu_college_physics@5': 0.45098039215686275,\n",
    " 'mmlu_computer_security@5': 0.77,\n",
    " 'mmlu_conceptual_physics@5': 0.5829787234042553,\n",
    " 'mmlu_electrical_engineering@5': 0.6344827586206897,\n",
    " 'mmlu_elementary_mathematics@5': 0.3968253968253968,\n",
    " 'mmlu_high_school_biology@5': 0.7645161290322581,\n",
    " 'mmlu_high_school_chemistry@5': 0.5369458128078818,\n",
    " 'mmlu_high_school_computer_science@5': 0.62,\n",
    " 'mmlu_high_school_mathematics@5': 0.40370370370370373,\n",
    " 'mmlu_high_school_physics@5': 0.4304635761589404,\n",
    " 'mmlu_high_school_statistics@5': 0.5462962962962963,\n",
    " 'mmlu_machine_learning@5': 0.4017857142857143,\n",
    " 'mmlu_err@5': 0.003835221836153209,\n",
    " 'mmlu_humanities_err@5': 0.006775368801783291,\n",
    " 'mmlu_formal_logic_err@5': 0.0442626668137991,\n",
    " 'mmlu_high_school_european_history_err@5': 0.03346409881055953,\n",
    " 'mmlu_high_school_us_history_err@5': 0.02675640153807897,\n",
    " 'mmlu_high_school_world_history_err@5': 0.02553010046023351,\n",
    " 'mmlu_international_law_err@5': 0.03581796951709282,\n",
    " 'mmlu_jurisprudence_err@5': 0.04236511258094633,\n",
    " 'mmlu_logical_fallacies_err@5': 0.03408997886857529,\n",
    " 'mmlu_moral_disputes_err@5': 0.024405173935783234,\n",
    " 'mmlu_moral_scenarios_err@5': 0.016232826818678495,\n",
    " 'mmlu_philosophy_err@5': 0.02540383297817961,\n",
    " 'mmlu_prehistory_err@5': 0.024659685185967284,\n",
    " 'mmlu_professional_law_err@5': 0.012760464028289299,\n",
    " 'mmlu_world_religions_err@5': 0.029547741687640038,\n",
    " 'mmlu_other_err@5': 0.007833424946677572,\n",
    " 'mmlu_business_ethics_err@5': 0.04902071300001974,\n",
    " 'mmlu_clinical_knowledge_err@5': 0.027134291628741727,\n",
    " 'mmlu_college_medicine_err@5': 0.036430371689585475,\n",
    " 'mmlu_global_facts_err@5': 0.04852365870939098,\n",
    " 'mmlu_human_aging_err@5': 0.031381476375754995,\n",
    " 'mmlu_management_err@5': 0.03989139859531769,\n",
    " 'mmlu_marketing_err@5': 0.02093019318517933,\n",
    " 'mmlu_medical_genetics_err@5': 0.04351941398892446,\n",
    " 'mmlu_miscellaneous_err@5': 0.013702643715368983,\n",
    " 'mmlu_nutrition_err@5': 0.025457756696667864,\n",
    " 'mmlu_professional_accounting_err@5': 0.02981549448368206,\n",
    " 'mmlu_professional_medicine_err@5': 0.027778298701545447,\n",
    " 'mmlu_virology_err@5': 0.03864139923699121,\n",
    " 'mmlu_social_sciences_err@5': 0.007734190961291264,\n",
    " 'mmlu_econometrics_err@5': 0.047036043419179864,\n",
    " 'mmlu_high_school_geography_err@5': 0.02777253333421898,\n",
    " 'mmlu_high_school_government_and_politics_err@5': 0.027171213683164545,\n",
    " 'mmlu_high_school_macroeconomics_err@5': 0.024468615241478916,\n",
    " 'mmlu_high_school_microeconomics_err@5': 0.029837962388291932,\n",
    " 'mmlu_high_school_psychology_err@5': 0.015630022970092455,\n",
    " 'mmlu_human_sexuality_err@5': 0.037683359597287434,\n",
    " 'mmlu_professional_psychology_err@5': 0.018690850273595284,\n",
    " 'mmlu_public_relations_err@5': 0.04309118709946459,\n",
    " 'mmlu_security_studies_err@5': 0.028535560337128448,\n",
    " 'mmlu_sociology_err@5': 0.025196929874827072,\n",
    " 'mmlu_us_foreign_policy_err@5': 0.03684529491774707,\n",
    " 'mmlu_stem_err@5': 0.008541772519633976,\n",
    " 'mmlu_abstract_algebra_err@5': 0.04648231987117316,\n",
    " 'mmlu_anatomy_err@5': 0.04244633238353228,\n",
    " 'mmlu_astronomy_err@5': 0.03782728980865469,\n",
    " 'mmlu_college_biology_err@5': 0.03551446610810826,\n",
    " 'mmlu_college_chemistry_err@5': 0.04988876515698589,\n",
    " 'mmlu_college_computer_science_err@5': 0.050161355804659205,\n",
    " 'mmlu_college_mathematics_err@5': 0.04725815626252606,\n",
    " 'mmlu_college_physics_err@5': 0.04951218252396262,\n",
    " 'mmlu_computer_security_err@5': 0.04229525846816505,\n",
    " 'mmlu_conceptual_physics_err@5': 0.03223276266711712,\n",
    " 'mmlu_electrical_engineering_err@5': 0.04013124195424385,\n",
    " 'mmlu_elementary_mathematics_err@5': 0.025197101074246494,\n",
    " 'mmlu_high_school_biology_err@5': 0.024137632429337703,\n",
    " 'mmlu_high_school_chemistry_err@5': 0.035083705204426656,\n",
    " 'mmlu_high_school_computer_science_err@5': 0.048783173121456316,\n",
    " 'mmlu_high_school_mathematics_err@5': 0.02991481234222763,\n",
    " 'mmlu_high_school_physics_err@5': 0.04042809961395634,\n",
    " 'mmlu_high_school_statistics_err@5': 0.033953227263757976,\n",
    " 'mmlu_machine_learning_err@5': 0.04653333146973647}\n",
    "```\n",
    "\n",
    "hqq 8 64\n",
    "```json\n",
    "{'mmlu@5': 0.654037886340977, 'mmlu_humanities@5': 0.6006376195536663, 'mmlu_formal_logic@5': 0.47619047619047616, 'mmlu_high_school_european_history@5': 0.7818181818181819, 'mmlu_high_school_us_history@5': 0.8235294117647058, 'mmlu_high_school_world_history@5': 0.8227848101265823, 'mmlu_international_law@5': 0.8264462809917356, 'mmlu_jurisprudence@5': 0.7407407407407407, 'mmlu_logical_fallacies@5': 0.7423312883435583, 'mmlu_moral_disputes@5': 0.7225433526011561, 'mmlu_moral_scenarios@5': 0.4122905027932961, 'mmlu_philosophy@5': 0.7266881028938906, 'mmlu_prehistory@5': 0.7253086419753086, 'mmlu_professional_law@5': 0.4921773142112125, 'mmlu_world_religions@5': 0.8070175438596491, 'mmlu_other@5': 0.7206308336015449, 'mmlu_business_ethics@5': 0.65, 'mmlu_clinical_knowledge@5': 0.7584905660377359, 'mmlu_college_medicine@5': 0.6473988439306358, 'mmlu_global_facts@5': 0.33, 'mmlu_human_aging@5': 0.695067264573991, 'mmlu_management@5': 0.8446601941747572, 'mmlu_marketing@5': 0.8589743589743589, 'mmlu_medical_genetics@5': 0.83, 'mmlu_miscellaneous@5': 0.80970625798212, 'mmlu_nutrition@5': 0.7973856209150327, 'mmlu_professional_accounting@5': 0.5, 'mmlu_professional_medicine@5': 0.6911764705882353, 'mmlu_virology@5': 0.572289156626506, 'mmlu_social_sciences@5': 0.7630809229769255, 'mmlu_econometrics@5': 0.49122807017543857, 'mmlu_high_school_geography@5': 0.8080808080808081, 'mmlu_high_school_government_and_politics@5': 0.8963730569948186, 'mmlu_high_school_macroeconomics@5': 0.6487179487179487, 'mmlu_high_school_microeconomics@5': 0.7352941176470589, 'mmlu_high_school_psychology@5': 0.8495412844036697, 'mmlu_human_sexuality@5': 0.7709923664122137, 'mmlu_professional_psychology@5': 0.7238562091503268, 'mmlu_public_relations@5': 0.7090909090909091, 'mmlu_security_studies@5': 0.7346938775510204, 'mmlu_sociology@5': 0.8805970149253731, 'mmlu_us_foreign_policy@5': 0.89, 'mmlu_stem@5': 0.5616872819536949, 'mmlu_abstract_algebra@5': 0.29, 'mmlu_anatomy@5': 0.6148148148148148, 'mmlu_astronomy@5': 0.7236842105263158, 'mmlu_college_biology@5': 0.7847222222222222, 'mmlu_college_chemistry@5': 0.46, 'mmlu_college_computer_science@5': 0.49, 'mmlu_college_mathematics@5': 0.33, 'mmlu_college_physics@5': 0.5, 'mmlu_computer_security@5': 0.85, 'mmlu_conceptual_physics@5': 0.6127659574468085, 'mmlu_electrical_engineering@5': 0.6413793103448275, 'mmlu_elementary_mathematics@5': 0.42857142857142855, 'mmlu_high_school_biology@5': 0.7870967741935484, 'mmlu_high_school_chemistry@5': 0.5320197044334976, 'mmlu_high_school_computer_science@5': 0.68, 'mmlu_high_school_mathematics@5': 0.42962962962962964, 'mmlu_high_school_physics@5': 0.44370860927152317, 'mmlu_high_school_statistics@5': 0.5555555555555556, 'mmlu_machine_learning@5': 0.44642857142857145, 'mmlu_err@5': 0.0037947049308790343, 'mmlu_humanities_err@5': 0.00678283136290411, 'mmlu_formal_logic_err@5': 0.04467062628403273, 'mmlu_high_school_european_history_err@5': 0.03225078108306289, 'mmlu_high_school_us_history_err@5': 0.026756401538078955, 'mmlu_high_school_world_history_err@5': 0.02485636418450323, 'mmlu_international_law_err@5': 0.0345727283691767, 'mmlu_jurisprudence_err@5': 0.042365112580946336, 'mmlu_logical_fallacies_err@5': 0.03436150827846917, 'mmlu_moral_disputes_err@5': 0.024105712607754307, 'mmlu_moral_scenarios_err@5': 0.01646320023811451, 'mmlu_philosophy_err@5': 0.02531176597542611, 'mmlu_prehistory_err@5': 0.024836057868294677, 'mmlu_professional_law_err@5': 0.0127686730761119, 'mmlu_world_religions_err@5': 0.030267457554898458, 'mmlu_other_err@5': 0.007736708380444025, 'mmlu_business_ethics_err@5': 0.047937248544110196, 'mmlu_clinical_knowledge_err@5': 0.026341480371118362, 'mmlu_college_medicine_err@5': 0.036430371689585475, 'mmlu_global_facts_err@5': 0.047258156262526045, 'mmlu_human_aging_err@5': 0.030898610882477515, 'mmlu_management_err@5': 0.03586594738573974, 'mmlu_marketing_err@5': 0.022801382534597542, 'mmlu_medical_genetics_err@5': 0.0377525168068637, 'mmlu_miscellaneous_err@5': 0.014036945850381385, 'mmlu_nutrition_err@5': 0.023015446877985655, 'mmlu_professional_accounting_err@5': 0.029827499313594685, 'mmlu_professional_medicine_err@5': 0.028064998167040094, 'mmlu_virology_err@5': 0.038515976837185335, 'mmlu_social_sciences_err@5': 0.007488141850953062, 'mmlu_econometrics_err@5': 0.04702880432049615, 'mmlu_high_school_geography_err@5': 0.028057791672989017, 'mmlu_high_school_government_and_politics_err@5': 0.021995311963644237, 'mmlu_high_school_macroeconomics_err@5': 0.024203665177902803, 'mmlu_high_school_microeconomics_err@5': 0.028657491285071977, 'mmlu_high_school_psychology_err@5': 0.01532856\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
