{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=5\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from linear import HiggsLinear\n",
    "\n",
    "def replace_with_higgs_linear(\n",
    "    model,\n",
    "    quantization_config=None,\n",
    "    current_key_name=None,\n",
    "    has_been_replaced=False,\n",
    "):\n",
    "    for name, module in model.named_children():\n",
    "        if current_key_name is None:\n",
    "            current_key_name = []\n",
    "        current_key_name.append(name)\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Check if the current key is not in the `linear_weights_not_to_quantize`\n",
    "            if \".\".join(current_key_name) in quantization_config:\n",
    "                in_features = module.in_features\n",
    "                out_features = module.out_features\n",
    "                higgs_d = quantization_config[\".\".join(current_key_name)]\n",
    "\n",
    "                model._modules[name] = HiggsLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    higgs_d,\n",
    "                    bias=module.bias is not None,\n",
    "                    dtype=module.weight.dtype,\n",
    "                )\n",
    "                has_been_replaced = True\n",
    "\n",
    "                # Store the module class in case we need to transpose the weight later\n",
    "                model._modules[name].source_cls = type(module)\n",
    "                # Force requires grad to False to avoid unexpected errors\n",
    "                model._modules[name].requires_grad_(False)\n",
    "        if len(list(module.children())) > 0:\n",
    "            _, has_been_replaced = replace_with_higgs_linear(\n",
    "                module,\n",
    "                quantization_config=quantization_config,\n",
    "                current_key_name=current_key_name,\n",
    "                has_been_replaced=has_been_replaced,\n",
    "            )\n",
    "        # Remove the last key for recursion\n",
    "        current_key_name.pop(-1)\n",
    "    return model, has_been_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439b967812074853bb220d2cd5a1ef7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    # attn_implementation=\"eager\",\n",
    "    device_map=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def build_layerwise_edenn_config(\n",
    "    edenn_d: Optional[int] = None, \n",
    "    blockwise_edenn_config: Optional[list[int]] = None,\n",
    "    layerwise_edenn_config: Optional[dict[str, int]] = None,\n",
    ") -> list[(int, int)]:\n",
    "    if layerwise_edenn_config is not None:\n",
    "        assert edenn_d is None and blockwise_edenn_config is None\n",
    "        return layerwise_edenn_config\n",
    "    \n",
    "    if blockwise_edenn_config is None:\n",
    "        assert edenn_d is not None\n",
    "        blockwise_edenn_config = [edenn_d for _ in range(32)]\n",
    "    \n",
    "    layer_names = [\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        f\"model.layers.{i}.{layer_name}\": blockwise_edenn_config[i]\n",
    "        for layer_name in layer_names\n",
    "        for i in range(len(blockwise_edenn_config))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "DIM = 1\n",
    "\n",
    "if DIM != -1:\n",
    "    model, _ = replace_with_higgs_linear(\n",
    "        model,\n",
    "        build_layerwise_edenn_config(DIM),\n",
    "    )\n",
    "    model = load_checkpoint_and_dispatch(\n",
    "        model,\n",
    "        \"~/models/higgs/Meta-Llama-3.1-8B.pt\",\n",
    "    ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**tokenizer(\"Hi!\", return_tensors='pt').to(\"cuda\"))[0].cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(\n",
    "    **tokenizer(\"Hi!\", return_tensors='pt').to(\"cuda\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        model(**tokenizer(\"Hi!\", return_tensors='pt').to(\"cuda\"))\n",
    "    \n",
    "    # benchmarking with jupyter macro\n",
    "    %timeit model(**tokenizer(\"Hi!\", return_tensors='pt').to(\"cuda\")); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
