{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "/nfs/scistore19/alistgrp/apanfero/linear-layer-compression/Andrei/kernels/higgs.py:26: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"higgs::higgs_matmat\")\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=5\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from linear import HiggsLinear\n",
    "\n",
    "def replace_with_higgs_linear(\n",
    "    model,\n",
    "    quantization_config=None,\n",
    "    current_key_name=None,\n",
    "    has_been_replaced=False,\n",
    "):\n",
    "    for name, module in model.named_children():\n",
    "        if current_key_name is None:\n",
    "            current_key_name = []\n",
    "        current_key_name.append(name)\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Check if the current key is not in the `linear_weights_not_to_quantize`\n",
    "            if \".\".join(current_key_name) in quantization_config:\n",
    "                in_features = module.in_features\n",
    "                out_features = module.out_features\n",
    "                higgs_d = quantization_config[\".\".join(current_key_name)]\n",
    "\n",
    "                model._modules[name] = HiggsLinear(\n",
    "                    in_features,\n",
    "                    out_features,\n",
    "                    higgs_d,\n",
    "                    bias=module.bias is not None,\n",
    "                    dtype=module.weight.dtype,\n",
    "                )\n",
    "                has_been_replaced = True\n",
    "\n",
    "                # Store the module class in case we need to transpose the weight later\n",
    "                model._modules[name].source_cls = type(module)\n",
    "                # Force requires grad to False to avoid unexpected errors\n",
    "                model._modules[name].requires_grad_(False)\n",
    "        if len(list(module.children())) > 0:\n",
    "            _, has_been_replaced = replace_with_higgs_linear(\n",
    "                module,\n",
    "                quantization_config=quantization_config,\n",
    "                current_key_name=current_key_name,\n",
    "                has_been_replaced=has_been_replaced,\n",
    "            )\n",
    "        # Remove the last key for recursion\n",
    "        current_key_name.pop(-1)\n",
    "    return model, has_been_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bee0e3773cb4386aac4f0b8e635e2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from accelerate import init_empty_weights\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\", torch_dtype=torch.float16, attn_implementation=\"eager\")\n",
    "\n",
    "original_weight = model.model.layers[0].self_attn.q_proj.weight.data.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def build_layerwise_edenn_config(\n",
    "    edenn_d: Optional[int] = None, \n",
    "    blockwise_edenn_config: Optional[list[int]] = None,\n",
    "    layerwise_edenn_config: Optional[dict[str, int]] = None,\n",
    ") -> list[(int, int)]:\n",
    "    if layerwise_edenn_config is not None:\n",
    "        assert edenn_d is None and blockwise_edenn_config is None\n",
    "        return layerwise_edenn_config\n",
    "    \n",
    "    if blockwise_edenn_config is None:\n",
    "        assert edenn_d is not None\n",
    "        blockwise_edenn_config = [edenn_d for _ in range(32)]\n",
    "    \n",
    "    layer_names = [\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\",\n",
    "        \"self_attn.o_proj\",\n",
    "        \"mlp.gate_proj\",\n",
    "        \"mlp.up_proj\",\n",
    "        \"mlp.down_proj\",\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        f\"model.layers.{i}.{layer_name}\": blockwise_edenn_config[i]\n",
    "        for layer_name in layer_names\n",
    "        for i in range(len(blockwise_edenn_config))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = replace_with_higgs_linear(\n",
    "    model,\n",
    "    build_layerwise_edenn_config(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:1466: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    \"/nfs/scistore19/alistgrp/apanfero/models/higgs/Meta-Llama-3.1-8B.pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>Hi! I'm new to this forum. I'm a 19 year old female who has\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**tokenizer(\"Hi!\", return_tensors='pt').to(\"cuda\"))[0].cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
