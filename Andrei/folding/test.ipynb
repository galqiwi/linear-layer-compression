{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b1d37418df44a4b6eb1067654e37b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5a9cc9786f4de7984cfe72e0d4814a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fusing layer norms:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccb60a049b0416d991531de674dd9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rotating:   0%|          | 0/32 [00:00<?, ?layer/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM\n",
    "from rotation_utils import fuse_layer_norms, rotate_model\n",
    "\n",
    "MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL, device_map=\"cpu\")\n",
    "\n",
    "fuse_layer_norms(model, DEVICE)\n",
    "rotate_model(model, DEVICE)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model = model.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/GPTAQ/.conda/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>Hi! I'm new to the forum. I'm an amateur photographer, and I'm currently\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    model.generate(**tokenizer(\"Hi!\", return_tensors='pt').to(\"cuda\"))[0].cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "@torch.no_grad()\n",
    "def llama_eval(model, dataloader, dev):\n",
    "    print('Evaluating ...')\n",
    "\n",
    "    nsamples = len(dataloader) \n",
    "\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    layers = model.model.layers\n",
    "\n",
    "    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n",
    "    model.model.rotary_emb = model.model.rotary_emb.to(dev)\n",
    "    layers[0] = layers[0].to(dev)\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = []\n",
    "    attention_masks = []\n",
    "    position_ids = []\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps.append(inp)\n",
    "            attention_masks.append(kwargs['attention_mask'])\n",
    "            position_ids.append(kwargs['position_ids'])\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch.to(dev))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    layers[0] = layers[0].cpu()\n",
    "    model.model.embed_tokens = model.model.embed_tokens.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for i in trange(len(layers), desc=f\"Evaluating layer-by-layer...\"):\n",
    "        layer = layers[i].to(dev)\n",
    "        for j in range(nsamples):\n",
    "            inps[j] = layer(inps[j], attention_mask=attention_masks[j], position_ids=position_ids[j])[0]\n",
    "        layers[i] = layer.cpu()\n",
    "        del layer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if model.model.norm is not None:\n",
    "        model.model.norm = model.model.norm.to(dev)\n",
    "    model.lm_head = model.lm_head.to(dev)\n",
    "\n",
    "    nlls = []\n",
    "    for i in range(nsamples):\n",
    "        hidden_states = inps[i]\n",
    "        if model.model.norm is not None:\n",
    "            hidden_states = model.model.norm(hidden_states)\n",
    "        lm_logits = model.lm_head(hidden_states)\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = (dataloader[i].to(dev))[:, 1:]\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        neg_log_likelihood = loss.float() * 8192\n",
    "        nlls.append(neg_log_likelihood)\n",
    "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * 8192))\n",
    "    print(ppl.item())\n",
    "\n",
    "    model.config.use_cache = use_cache\n",
    "    \n",
    "    return ppl.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/scistore19/alistgrp/apanfero/linear-layer-compression/Andrei/folding/../gptq/edenn.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  GRIDS[dim][size] = torch.load(file)\n"
     ]
    }
   ],
   "source": [
    "from higgs import quantize_layer_higgs\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def quantize_model_higgs(model, higgs_d: int, higgs_n: int):\n",
    "    for layer in model.model.layers:\n",
    "        layer.self_attn.q_proj.weight.data = quantize_layer_higgs(layer.self_attn.q_proj.weight.data, 1, higgs_d, higgs_n)\n",
    "        layer.self_attn.k_proj.weight.data = quantize_layer_higgs(layer.self_attn.k_proj.weight.data, 1, higgs_d, higgs_n)\n",
    "        layer.self_attn.v_proj.weight.data = quantize_layer_higgs(layer.self_attn.v_proj.weight.data, 1, higgs_d, higgs_n)\n",
    "        layer.self_attn.o_proj.weight.data = quantize_layer_higgs(layer.self_attn.o_proj.weight.data, 0, higgs_d, higgs_n)\n",
    "        \n",
    "        layer.mlp.gate_proj.weight.data = quantize_layer_higgs(layer.mlp.gate_proj.weight.data, 1, higgs_d, higgs_n)\n",
    "        layer.mlp.up_proj.weight.data = quantize_layer_higgs(layer.mlp.up_proj.weight.data, 1, higgs_d, higgs_n)\n",
    "        layer.mlp.down_proj.weight.data = quantize_layer_higgs(layer.mlp.down_proj.weight.data, 0, higgs_d, higgs_n)\n",
    "        \n",
    "quantize_model_higgs(model, 2, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2436214 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208cb7183ae9406292f89f9f0fda5b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating layer-by-layer...:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.358399868011475\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from gptq.datautils import get_loaders\n",
    "\n",
    "datasets = ['wikitext2'] \n",
    "for dataset in datasets:\n",
    "    dataloader, testloader = get_loaders(\n",
    "        dataset, seed=0, model=MODEL, seqlen=8192\n",
    "    )\n",
    "    ppl = llama_eval(model, testloader, \"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki-2 PPL\n",
    "\n",
    "FP32: `5.60677433013916`\n",
    "\n",
    "FP16: `5.606886386871338`\n",
    "\n",
    "MR -> FP32: `5.607339382171631`\n",
    "\n",
    "MR -> FP16: `5.60782527923584`\n",
    "\n",
    "MR -> HIGGS 2d256: `6.358399868011475`\n",
    "\n",
    "OR -> HIGGS 2d256: `6.015`\n",
    "\n",
    "\n",
    ", MR: Merger Rotations, OR: Online Rotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c999ea615174a8483b1f9fb8adc4f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ref_model = LlamaForCausalLM.from_pretrained(MODEL, device_map=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hadamard import random_hadamard_matrix\n",
    "\n",
    "Q = random_hadamard_matrix(4096, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007672099746861981"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized = model.model.layers[0].self_attn.q_proj.weight.data.clone().detach().cuda()\n",
    "\n",
    "ref = torch.matmul(\n",
    "    (ref_model.model.layers[0].self_attn.q_proj.weight.data * ref_model.model.layers[0].input_layernorm.weight.data[None,:]).double().cuda(),\n",
    "    Q,\n",
    ")\n",
    "\n",
    "float((quantized - ref).pow(2).sum() / ref.pow(2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007760077692277988"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized = model.model.layers[0].self_attn.o_proj.weight.data.clone().detach().cuda()\n",
    "\n",
    "ref = torch.matmul(\n",
    "    Q.T,\n",
    "    (ref_model.model.layers[0].self_attn.o_proj.weight.data).double().cuda(),\n",
    ")\n",
    "\n",
    "float((quantized - ref).pow(2).sum() / ref.pow(2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00774175502160158"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized = model.model.layers[0].mlp.gate_proj.weight.data.clone().detach().cuda()\n",
    "\n",
    "ref = torch.matmul(\n",
    "    (ref_model.model.layers[0].mlp.gate_proj.weight.data * ref_model.model.layers[0].post_attention_layernorm.weight.data[None,:]).double().cuda(),\n",
    "    Q,\n",
    ")\n",
    "\n",
    "float((quantized - ref).pow(2).sum() / ref.pow(2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007750276103880045"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized = model.model.layers[0].mlp.down_proj.weight.data.clone().detach().cuda()\n",
    "\n",
    "ref = torch.matmul(\n",
    "    Q.T,\n",
    "    (ref_model.model.layers[0].mlp.down_proj.weight.data).double().cuda(),\n",
    ")\n",
    "\n",
    "float((quantized - ref).pow(2).sum() / ref.pow(2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
